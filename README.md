
# Метрические алгоритмы классификации
**Метрические методы обучения** -- методы, основанные на анализе сходства объектов.

**_Мерой близости_** называют функцию расстояния ![](http://latex.codecogs.com/svg.latex?%5Clarge%20%5Crho%3A%20%28X%20%5Ctimes%20X%29%20%5Crightarrow%20%5Cmathbb%7BR%7D). Чем меньше расстояние между объектами, тем больше объекты похожи друг на друга.

Метрические алгоритмы классификации опираются на **_гипотезу компактности_**: схожим объектам соответствуют схожие ответы.

Метрические алгоритмы классификации с обучающей выборкой *Xl* относят объект *u* к тому классу *y*, для которого **суммарный вес ближайших обучающих объектов ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) максимален**:

![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29%20%3D%20%5Csum_%7Bi%20%3A%20y_%7Bu%7D%5E%7B%28i%29%7D%20%3D%20y%7D%20w%28i%2C%20u%29%20%5Crightarrow%20max)

, где весовая функция *w(i, u)* оценивает степень важности *i*-го соседа для классификации объекта *u*.

Функция ![](https://latex.codecogs.com/gif.latex?W_y%28u%2C%20X%5El%29) называется **_оценкой близости объекта u к классу y_**. Выбирая различную весовую функцию *w(i, u)* можно получать различные метрические классификаторы.

Для поиска оптимальных параметров для каждого из рассматриваемых ниже метрических алгоритмов используется **LOO -- leave-one-out** *(критерий скользящего контроля)*, который состоит в следующем: 

1. Исключать объекты *x(i)* из выборки *Xl* по одному, получится новая выборка без объекта *x(i)* (назовём её *Xl_1*).
2. Запускать алгоритм от объекта *u*, который нужно классифицировать, на выборке *Xl_1*.
3. Завести переменную *Q* (накопитель ошибки, изначально *Q = 0*) и, когда алгоритм ошибается, *Q = Q + 1*.
4. Когда все объекты *x(i)* будут перебраны, вычислить *LOO = Q / l* (*l* -- количество объектов выборки).

При минимальном значении LOO получим оптимальный параметр алгоритма.

### Алгоритм 1NN и K ближайших соседей (KNN)

**1NN:**

1. Подбирается метрика. В данной работе это декартово расстояние между векторами.
2. Обучающая выборка сортируется в порядке увеличения расстояния от классифицируемого элемента.
3. Элемент относят к тому классу к которому принадлежит ближайший (первый в отсортированной выборке) элемент.

**КNN:**

Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера).
Данный алгоритм классификации относит классифицируемый объект *u* к тому классу *y*, к которому относится большинство из *k* его ближайших соседей *x(u_i)*.
Для оценки близости классифицируемого объекта *u* к классу *y* **алгоритм kNN** использует следующую функцию:
![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D) , где *i* -- порядок соседа по расстоянию к классифицируемому объекту *u*.

Реализация алгоритмов:

``` R
par(mfrow=c(2,1))
selected = data[c(1,3,5)]
features = dim(selected)[2]-1
cases = dim(selected)[1]
colors = c("red", "green", "blue")

dist = function(u, v) { #это возвращает эвклидовое расстояние между двумя объектами
  sqrt(sum((u-v)^2))
}

distances = function(obj, data, metric) { # это возвращает отсортированный набор данных по метрике для объекта  
dists = matrix(0, cases, 2)
  for (i in 1:cases) {
    cost = metric(obj, data[i,1:features])
    dists[i,] = c(cost, i)
  }
  idx = order(dists[,1])
  data[dists[idx,2],]
}

NN = function(obj, data, metric=dist) { # это 1-ближайший сосед  
sorted = distances(obj, data, metric)
  sorted[1,features+1]
}

kNN = function(obj, data, k, metric=dist) { # это k-ближайших соседей  
sorted = distances(obj, data, metric)
  
  n = 10 
  counts = rep(0, times=n)
  for (i in 1:k) {
    cls = sorted[i,features+1]
    counts[cls] = counts[cls] + 1
  }
  argmax = 1
  for (i in n) {
    if (counts[argmax] < counts[i]) {
      argmax = i
    }
  }
  
  cls[argmax]
}


points = rbind(#классификация 
  c(5.5, 2),
  c(6.5, 4),
  c(7, 6.5),
  c(5.4, 2.5)
)


# 1NN
plot(selected[,1], selected[,2], col=colors[selected[,features+1]], xlab="1NN", ylab="")
for (i in 1:dim(points)[1]) {
  pt = points[i,]
  points(pt[1], pt[2], col=colors[NN(pt, selected)], pch=19) 
}

# kNN
plot(selected[,1], selected[,2], col=colors[selected[,features+1]], xlab="kNN", ylab="")
for (i in 1:dim(points)[1]) {
  pt = points[i,]
  points(pt[1], pt[2], col=colors[kNN(pt, selected, 7)], pch=19) 
}
```
Вот что получилось:
![alt_text](https://github.com/pyatixatka/956/blob/master/Metricheskie%20klassifikatoru/1_NN_kNN/rezultrisunok.png)
Алгоритм kNN выглядит более качествеено. Для того чтобы привести более точное обоснование чем kNN лучше в этом случае, чем 1NN, следует прибегнуть к скользящему контролю.

**LOO для КNN:**

Посмотрим как отработал KNN при помощи алгоритма скользящего конторля - LOO.
![alt text](https://github.com/pyatixatka/956/blob/master/Metricheskie%20klassifikatoru/2_Loo_kNN/rezultat.png)

Минимальный LOO достигается при k=6

Програмная реализация:
``` R
getLoo = function(x) {
  l = dim(x)[1]
  n = dim(x)[2] - 1
  maxk = l
  
  loo = rep(0, times=maxk)
  
  for (i in 1:l) {
    dists = distances(x[i,], x[-i,], dist)
    for (k in 1:maxk) {
      class = applykNN(dists, k)
      if (as.integer(class) != as.integer(x[i,n+1])) {
        loo[k] = loo[k] + 1
      }
    }
    print(i)
    print(loo)
  }
  loo = loo / l
  return(loo)
}
which.min(res)
# res = getLoo(selected)
```
### Преимущества:
1. Простота реализации.
2. При *k*, подобранном около оптимального, алгоритм "неплохо" классифицирует.

### Недостатки:
1. Нужно хранить всю выборку.
2. При *k = 1* неустойчивость к погрешностям (*выбросам* -- объектам, которые окружены объектами чужого класса), вследствие чего этот выброс классифицировался неверно и окружающие его объекты, для которого он окажется ближайшим, тоже.
2. При *k = l* алгоритм наоборот чрезмерно устойчив и вырождается в константу.
3. Максимальная сумма объектов в *counts* может достигаться в нескольких классах одновременно.
4. "Скудный" набор параметров.
5. Точки, расстояние между которыми одинаково, не все будут учитываться.

### K взвешенных ближайших соседей (kwKNN)
Имеется некоторая выборка *Xl*, состоящая из объектов *x(i), i = 1, ..., l* (в приложенной программе используется выборка ирисов Фишера).
Данный алгоритм классификации относит объект *u* к тому классу *y*, у которого максимальна сумма весов *w_i* его ближайших *k* соседей *x(u_i)*.

Для оценки близости классифицируемого объекта *u* к классу *y* **алгоритм wkNN** использует следующую функцию:

![](http://latex.codecogs.com/svg.latex?%5Clarge%20W%28i%2C%20u%29%20%3D%20%5Bi%20%5Cleq%20k%5D%20w%28i%29) , где *i* -- порядок соседа по расстоянию к классифицируемому объекту *u*, а *w(i)* -- строго убывающая функция веса, задаёт вклад i-го соседа в классификацию.

В приложенной программе используется весовая функция вида: ![](https://latex.codecogs.com/gif.latex?w%28i%29%20%3D%20q%5Ei%2C%20q%20%5Cepsilon%20%280%2C%201%29)

Реализация kwKNN производится следующим образом:
``` R
kwNN = function(u, xl, k, q, ro) {
  l = dim(xl)[1]
  n = dim(xl)[2]-1
  sorted = distances(u, xl, ro)
  facts = levels(xl[,n+1])
  scores = rep(0, times=length(facts))
  curr = 1
  for (i in 1:k) {
    currclass = sorted[i,n+1]
    scores[currclass] = scores[currclass] + curr
    curr = curr * q
  }
  max = which.max(scores)
  factor(facts[max], levels=facts)
}
```
Как в kNN внутри цикла получим отсортированных соседей, только теперь у нас будет ещё и множитель curr, который будет умножаться на i-того соседа. Во время каждой итерации curr = q^i.

Подобираем параметр q, k
Взяли k = 6, так как он показал себя наилучший LOO в kNN, а параметр q подбирать через LOO.
Наилучшим q оказался 1 с loo(q) = 0.0333.

Посмотрим как отработал kwKNN

![alt text](https://github.com/pyatixatka/956/blob/master/Metricheskie%20klassifikatoru/kwNN/map.png)

Из-за того, что **алгоритм k взвешенных ближайших соседей** учитывает порядок объектов при классификации, он выдаёт лучший результат, чем **алгоритм k ближайших соседей (kNN)**. Следовательно, объекты *x_i*, которые находятся ближе к классифицируемому объекту *u*, будут оказывать намного большее влияние, чем те *x_i*, которые дальше (из-за учёта порядка объектов).

### Преимущества:
1. Простота реализации.
2. При любом *k* алгоритм "неплохо" классифицирует.

### Недостатки:
1. Приходится хранить обучающую выборку *Xl* целиком, что приводит к неэффективному расходу памяти. При наличии погрешностей это может привести к понижению точности классификации вблизи границ классов.
2. Поиск ближайшего соседа предполагает сравнение классифицируемого объекта *u* со всеми объектами выборки *Xl (x(i), i = 1, ..., l)* за *O(l)* операций. Эта проблема решается с помощью эффективных алгоритмов за *O(lnl)*.
3. Исключается настройка алгоритмов по данным (крайне "бедный" набор параметров).
4. Если суммарные веса классов оказываются одинаковыми, то алгоритм относит классифицируемый объект *u* к любому из классов.

## Байесовские классификаторы

**Байесовские алгоритмы классификации** основаны на предположении, что есть вероятностное пространство ![](https://latex.codecogs.com/gif.latex?X%20%5Ctimes%20Y) с неизвестной плотностью распределения ![](https://latex.codecogs.com/gif.latex?%5Crho%20%28x%2C%20y%29%20%3D%20P%28y%29%5Crho%20%28x%20%7C%20y%29), из которого случайно и независимо извлекаются *l* наблюдений.

Байесовский подход опирается на теорему о том, что **если плотности распределения классов известны, то алгоритм классификации, имеющий минимальную вероятность ошибок, можно выписать в явном виде**.

Обозначим *величину потери* алгоритмом *а* при неправильной классификации объекта класса *y* ![](https://latex.codecogs.com/gif.latex?%5Clambda%20_%7By%7D).

**Теорема:** Если известны априорные вероятности классов *P(y)* и функции правдоподобия *p(x*|*y)*, то минимум среднего риска достигается алгоритмом ![](https://latex.codecogs.com/gif.latex?a%28x%29%20%3D%20arg%20%5Cmax_%7By%20%5Cepsilon%20Y%7D%5Clambda%20_%7By%7DP%28y%29%5Crho%28x%7Cy%29). Алгоритм *a(x)* называется **оптимальным байесовским решающим правилом**.

На практике зачастую плотности распределения классов неизвестны и их приходится восстанавливать по обучающей выборке. **Чем лучше удастся восстановить функции правдоподобия, тем ближе к оптимальному будет построенный алгоритм**.

В зависимости от способов восстановления плотности существует большое разнообразие **байесовских алгоритмов классификации**.

### "Наивный" байесовский классификатор

Предполагается, что все объекты обучающей выборки *X* описываются *n* числовыми признаками ![](https://latex.codecogs.com/gif.latex?f_j%2C%20j%20%3D%201%2C%20...%2C%20n). Все эти признаки -- независимые случайные величины. Тогда функции правдоподобия классов представимы в виде:

![](https://latex.codecogs.com/gif.latex?%5Crho_y%28x%29%20%3D%20%5Crho_%7By_%7B1%7D%7D%28f_1%28x%29%29%5Ccdot%20...%20%5Ccdot%20%5Crho_%7By_%7Bn%7D%7D%28f_n%28x%29%29%2C%20y%20%5Cepsilon%20Y) ,

где ![](https://latex.codecogs.com/gif.latex?%5Crho_%7By_%7Bj%7D%7D%28f_j%28x%29%29) -- плотность распределения значений *j*-го признака для класса *y*.

Данный алгоритм основан на предположении, что *n* одномерных плотностей оценить проще, чем одну *n*-мерную, но на практике это редко получается.

Эмпирическая оценка *n*-мерной плотности находится следующим образом:

![](https://latex.codecogs.com/gif.latex?%5Chat%7Bp%7D_h%28x%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%20%5Csum_%7Bi%20%3D%201%7D%5E%7Bm%7D%20%5Cprod_%7Bj%20%3D%201%7D%5E%7Bn%7D%20%5Cfrac%7B1%7D%7Bh_j%7D%20K%20%5Cleft%28%20%5Cfrac%7Bf_j%28x%29%20-%20f_j%28x_i%29%29%29%7D%7Bh_j%7D%20%5Cright%20%29) ,

где *x* -- классифицируемая точка, *x_i* -- точки обучающей выборки, *h* -- ширина окна, *m* -- кол-во точек выборки, *n* -- кол-во признаков, *K* -- функция ядра, *f_j* -- признаки.

Подставив эту оценку в оптимальное байесовское решающее правило, получим **"наивный" байесовский классификатор**.

В качестве обучающей выборки *X* рассматриваются ирисы Фишера, ядро *K* выбрано Гауссовское.

Имея мат. ожидания и среднеквадратичные отклонения решающее правило можно написать так:
``` R
a = function(x, classes, probs, mus, stds) {
  Y = length(classes)
  n = dim(mus)[2]
  scores = rep(0, Y)
  for (i in 1:Y) {
    scores[i] = probs[i]
    for (j in 1:n) {
      scores[i] = scores[i] * N(x[j], mus[i,j], stds[i,j])
    }
  }
  res = which.max(scores)
  factor(classes[res], levels=classes)
}
``` 
Карта классификации:
![](https://github.com/pyatixatka/956/blob/master/Bayusovskie%20klassifikatoru/NaiveBayesClass/classmap2.png)

Видно, что граница принятия решения гладкая, что очень хорошо, однако можно видеть неблагоприятные решения отмеченные красным цветом. Наивный байесовский классификатор показал ошибку 4% на 2х признаках ирисов Фишера.

Вывод: Наивный байесовский классификатор это алгоритм, используемый для тестирования точности других алгоритмов. Он прост, чрезвычайно быстр и создает очень гладкие границы принятия решения. Он очень портативный, что позволяет быстро использовать его в любом языке.
